{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63785634",
   "metadata": {},
   "source": [
    "# Power your products with ChatGPT and your own data using Notion Reader API\n",
    "\n",
    "This is a walkthrough taking readers through how to build starter Q&A and Chatbot applications using the ChatGPT API and their own data. \n",
    "\n",
    "It is laid out in these sections:\n",
    "- **Setup:** \n",
    "    - Initiate variables and source the data\n",
    "- **Lay the foundations:**\n",
    "    - Load notion documents into LlamaIndex for vectorizing\n",
    "    - Save documents in Redis\n",
    "- **Make it a product:**\n",
    "    - Add a retrieval step where users provide queries and we return the most relevant entries\n",
    "    - Summarise search results with GPT-3\n",
    "    - Test out this basic Q&A app in Streamlit\n",
    "- **Build your moat:**\n",
    "    - Create an Assistant class to manage context and interact with our bot\n",
    "    - Use the Chatbot to answer questions using semantic search context\n",
    "    - Test out this basic Chatbot app in Streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f08ea7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T18:21:08.507374Z",
     "start_time": "2023-06-06T18:21:08.492799Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ae815d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb963fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T18:26:44.810697Z",
     "start_time": "2023-06-06T18:26:43.918859Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r ./../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13649895",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First we'll setup our libraries and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7590fbfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T18:27:08.367136Z",
     "start_time": "2023-06-06T18:27:07.966437Z"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "from database import get_redis_connection\n",
    "\n",
    "# Set our default models and chunking size\n",
    "from config import COMPLETIONS_MODEL, CHAT_MODEL\n",
    "\n",
    "# Ignore unclosed SSL socket warnings - optional in case you get these errors\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"unclosed\", category=ImportWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760efc1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T18:27:09.287331Z",
     "start_time": "2023-06-06T18:27:09.283994Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc4018c",
   "metadata": {},
   "source": [
    "## Laying the foundations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632b82ed",
   "metadata": {},
   "source": [
    "### Storage\n",
    "\n",
    "We're going to use Redis as our database for both document contents and the vector embeddings. You will need the full Redis Stack to enable use of Redisearch, which is the module that allows semantic search - more detail is in the [docs for Redis Stack](https://redis.io/docs/stack/get-started/install/docker/).\n",
    "\n",
    "To set this up locally, you will need to install Docker and then run the following command: ```docker run -d --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest```.\n",
    "\n",
    "The code used here draws heavily on [this repo](https://github.com/RedisAI/vecsim-demo).\n",
    "\n",
    "After setting up the Docker instance of Redis Stack, you can follow the below instructions to initiate a Redis connection and create a Hierarchical Navigable Small World (HNSW) index for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Setup Redis and running?\n",
    "from database import get_redis_connection\n",
    "\n",
    "redis_client = get_redis_connection()\n",
    "\n",
    "redis_client.ping()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T18:27:16.030700Z",
     "start_time": "2023-06-06T18:27:16.006391Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3ad41f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T18:27:18.133540Z",
     "start_time": "2023-06-06T18:27:18.131305Z"
    }
   },
   "outputs": [],
   "source": [
    "# Optional step to drop the index if it already exists\n",
    "from config import INDEX_NAME\n",
    "\n",
    "# redis_client.ft(INDEX_NAME).dropindex()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74ebeb5",
   "metadata": {},
   "source": [
    "### Ingestion\n",
    "\n",
    "We'll load up our PDF pages into documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed23bf9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T18:27:20.219001Z",
     "start_time": "2023-06-06T18:27:20.217795Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3912ecdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T18:27:25.097679Z",
     "start_time": "2023-06-06T18:27:23.592895Z"
    }
   },
   "outputs": [],
   "source": [
    "from llama_index import NotionPageReader, SimpleDirectoryReader\n",
    "from config import NOTION_API_KEY\n",
    "import os\n",
    "\n",
    "# integration_token = os.getenv(\"NOTION_INTEGRATION_TOKEN\")\n",
    "# integration_token = NOTION_API_KEY\n",
    "\n",
    "# page_ids = [\"76d816d82434423d8fbec83a3979d245\", #AI Knowledge Base\n",
    "#             \"40b801917ab04deb8f5759d9d3e2da59\", #The Chicago Office\n",
    "#             \"64c61657f90b48f786e8b55098f26e3a\", #Denver Lightning Talks\n",
    "#             \"642768dbfd6041e699a24b2863fab5b2\", #Denver IRL Agenda\n",
    "#             \"9f45258c6cab4592badeec6f1060e5df\", #Pairing\n",
    "#             \"c4e0d82f59d444c486066205919e2088\", #Why we do what we do\n",
    "#             \"5b25eda9934745fa8e9dd4bbf131eabc\", #Chicago IRL Logistics\n",
    "#             \"4747650f9fd74f9b9d43e817963d6759\", #Denver IRL Logistics\n",
    "#             \"53f3ff1456ea4c298e70281902080d9f\", #Pairing Interview\n",
    "#             \"f621f275876945dcab7a298d21fb95c4\", #Denver Family Friend Day\n",
    "#             \"2bcdac9fedd14404ba85a47323f5a1ad\", #Tech Lead\n",
    "#             \"87d80ebf66c64422a21570b5c2d0b0bc\", #Daily Company Stand up\n",
    "#             \"a255954246c44ab1a6178f54f1095b41\", #Pair Retros\n",
    "#             \"0050c37a71464a73ab77e01a5ab5a76d\", #Project Rotations\n",
    "#             \"6977e3c091384b1abe1f1692fce9995f\", #2023 Strategy\n",
    "#             \"ff2478fce704496d85462d8c8656f074\", #The Denver Office\n",
    "#             \"d30c87af7d60458aaf7fb412d422a691\", #2023 Chicago IRL\n",
    "#             \"690037a4c2a64688b43303bc4d2a65d0\", #Product Design Lunch\n",
    "#             \"5ade11d0a39342129580364d35106eee\", #Ski Weekend\n",
    "#             # \"50901371968b4dbb9423186c7cd392dc\", #Software Development - Issues?\n",
    "#             \"dc8864d18f7c4c1595c2e278d36743c4\", #So you want to be a TPIer?\n",
    "#             # \"447552ca7fec4e2fa568cb918d166c42\", #Anchors - Issues?\n",
    "#             \"94f952deb00740929e9b526f93609c46\", #Denver activities and meals\n",
    "#             \"8ea1eccd201842c28f9cd709b95754a1\"] #Chicago IRL Agenda\n",
    "#\n",
    "# documents = NotionPageReader(integration_token=integration_token).load_data(page_ids=page_ids)\n",
    "\n",
    "documents = SimpleDirectoryReader(input_dir=\"/Users/skainec/Documents/Workspace/aiknowledgehubdemo/data\").load_data()\n",
    "# index = VectorStoreIndex.from_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import sanitize_text\n",
    "\n",
    "for document in documents:\n",
    "    document.text = sanitize_text(document.text)\n",
    "    print(document.text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T18:27:29.676006Z",
     "start_time": "2023-06-06T18:27:29.663854Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "documents[0].text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T18:27:50.999202Z",
     "start_time": "2023-06-06T18:27:50.995798Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex\n",
    "from llama_index.vector_stores import RedisVectorStore\n",
    "from config import OPENAI_API_KEY, INDEX_NAME, PREFIX\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "vector_store = RedisVectorStore(\n",
    "    index_name=INDEX_NAME,\n",
    "    index_prefix=PREFIX,\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    "    overwrite=True,\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = GPTVectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
    "\n",
    "vector_store.persist(persist_path=\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T18:28:36.013731Z",
     "start_time": "2023-06-06T18:28:28.890793Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is the address of the Chicago office?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T15:35:30.261548Z",
     "start_time": "2023-06-07T15:35:20.748554Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T15:35:32.434912Z",
     "start_time": "2023-06-07T15:35:32.433323Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "dd12b31e",
   "metadata": {},
   "source": [
    "## Build your moat\n",
    "\n",
    "The Q&A was useful, but fairly limited in the complexity of interaction we can have - if the user asks a sub-optimal question, there is no assistance from the system to prompt them for more info or conversation to lead them down the right path.\n",
    "\n",
    "For the next step we'll make a Chatbot using the Chat Completions endpoint, which will:\n",
    "- Be given instructions on how it should act and what the goals of its users are\n",
    "- Be supplied some required information that it needs to collect\n",
    "- Go back and forth with the customer until it has populated that information\n",
    "- Say a trigger word that will kick off semantic search and summarisation of the response\n",
    "\n",
    "For more details on our Chat Completions endpoint and how to interact with it, please check out the docs [here](https://platform.openai.com/docs/guides/chat)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34135886",
   "metadata": {},
   "source": [
    "### Framework\n",
    "\n",
    "This section outlines a basic framework for working with the API and storing context of previous conversation \"turns\". Once this is established, we'll extend it to use our retrieval endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c0acc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T21:20:10.820097Z",
     "start_time": "2023-06-05T21:20:08.838648Z"
    }
   },
   "outputs": [],
   "source": [
    "# A basic example of how to interact with our ChatCompletion endpoint\n",
    "# It requires a list of \"messages\", consisting of a \"role\" (one of system, user or assistant) and \"content\"\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "question = 'How can you help me'\n",
    "\n",
    "new_vector_store = RedisVectorStore(\n",
    "    index_name=INDEX_NAME,\n",
    "    index_prefix=PREFIX,\n",
    "    redis_url=\"redis://localhost:6379\"\n",
    ")\n",
    "\n",
    "new_storage_context = StorageContext.from_defaults(vector_store=new_vector_store)\n",
    "new_index = GPTVectorStoreIndex([], storage_context=new_storage_context)\n",
    "\n",
    "query_engine = new_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is the 2023 strategy?\")\n",
    "response.response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T22:32:24.488049Z",
     "start_time": "2023-06-05T22:32:21.691188Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T22:25:49.504729Z",
     "start_time": "2023-06-05T22:25:49.501079Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e4fc55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T18:56:23.251472Z",
     "start_time": "2023-06-01T18:56:23.231515Z"
    }
   },
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "# A basic class to create a message as a dict for chat\n",
    "class Message:\n",
    "    \n",
    "    \n",
    "    def __init__(self,role,content):\n",
    "        \n",
    "        self.role = role\n",
    "        self.content = content\n",
    "        \n",
    "    def message(self):\n",
    "        \n",
    "        return {\"role\": self.role,\"content\": self.content}\n",
    "        \n",
    "# Our assistant class we'll use to converse with the bot\n",
    "class Assistant:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def _get_assistant_response(self, prompt):\n",
    "        \n",
    "        try:\n",
    "            completion = openai.ChatCompletion.create(\n",
    "              model=\"gpt-3.5-turbo\",\n",
    "              messages=prompt\n",
    "            )\n",
    "            \n",
    "            response_message = Message(completion['choices'][0]['message']['role'],completion['choices'][0]['message']['content'])\n",
    "            return response_message.message()\n",
    "            \n",
    "        except Exception as e:\n",
    "            \n",
    "            return f'Request failed with exception {e}'\n",
    "\n",
    "    def ask_assistant(self, next_user_prompt, colorize_assistant_replies=True):\n",
    "        [self.conversation_history.append(x) for x in next_user_prompt]\n",
    "        assistant_response = self._get_assistant_response(self.conversation_history)\n",
    "        self.conversation_history.append(assistant_response)\n",
    "        return assistant_response\n",
    "            \n",
    "        \n",
    "    def pretty_print_conversation_history(self, colorize_assistant_replies=True):\n",
    "        for entry in self.conversation_history:\n",
    "            if entry['role'] == 'system':\n",
    "                pass\n",
    "            else:\n",
    "                prefix = entry['role']\n",
    "                content = entry['content']\n",
    "                output = colored(prefix +':\\n' + content, 'green') if colorize_assistant_replies and entry['role'] == 'assistant' else prefix +':\\n' + content\n",
    "                print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18c88b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T18:56:29.013533Z",
     "start_time": "2023-06-01T18:56:28.989882Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initiate our Assistant class\n",
    "conversation = Assistant()\n",
    "\n",
    "# Create a list to hold our messages and insert both a system message to guide behaviour and our first user question\n",
    "messages = []\n",
    "system_message = Message('system','You are a helpful business assistant who has innovative ideas')\n",
    "user_message = Message('user','What can you do to help me')\n",
    "messages.append(system_message.message())\n",
    "messages.append(user_message.message())\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377243c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T18:56:44.799028Z",
     "start_time": "2023-06-01T18:56:38.267890Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get back a response from the Chatbot to our question\n",
    "response_message = conversation.ask_assistant(messages)\n",
    "print(response_message['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f364c3b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T18:57:07.379876Z",
     "start_time": "2023-06-01T18:56:58.615943Z"
    }
   },
   "outputs": [],
   "source": [
    "next_question = 'Tell me more about option 2'\n",
    "\n",
    "# Initiate a fresh messages list and insert our next question\n",
    "messages = []\n",
    "user_message = Message('user',next_question)\n",
    "messages.append(user_message.message())\n",
    "response_message = conversation.ask_assistant(messages)\n",
    "print(response_message['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62842a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T18:57:16.346774Z",
     "start_time": "2023-06-01T18:57:16.312596Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print out a log of our conversation so far\n",
    "\n",
    "conversation.pretty_print_conversation_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d5b54",
   "metadata": {},
   "source": [
    "### Knowledge retrieval\n",
    "\n",
    "Now we'll extend the class to call a downstream service when a stop sequence is spoken by the Chatbot.\n",
    "\n",
    "The main changes are:\n",
    "- The system message is more comprehensive, giving criteria for the Chatbot to advance the conversation\n",
    "- Adding an explicit stop sequence for it to use when it has the info it needs\n",
    "- Extending the class with a function ```_get_search_results``` which sources Redis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0cef87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T18:57:37.578792Z",
     "start_time": "2023-06-01T18:57:37.547179Z"
    }
   },
   "outputs": [],
   "source": [
    "from config import OPENAI_API_KEY\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Updated system prompt requiring Question and Year to be extracted from the user\n",
    "system_prompt = '''\n",
    "You are a helpful virtual assistant for the employees of Focused Labs. Focused Labs is a boutique Software Consulting firm that specializes in enterprise application development and digital transformation. Employees will ask you questions about the inner workings of the company. Questions could range in areas such as process, procedure, policy, and culture. Employees have different roles. The roles are either Developer, Designer, or Product Manager. The question is about how the company of Focused Labs operates. For each question, you need to capture their role.\n",
    "If they haven't provided their role, ask them for it.\n",
    "Once you have their role, say \"let me check on that for you...\".\n",
    "\n",
    "Example 1:\n",
    "\n",
    "User: I'd like to know how many IRLs Focused Labs has hosted\n",
    "\n",
    "Assistant: Certainly, what is your role at the Company?\n",
    "\n",
    "User: I am a designer.\n",
    "\n",
    "Assistant: let me check on that for you...\n",
    "'''\n",
    "\n",
    "# New Assistant class to add a vector database call to its responses\n",
    "class RetrievalAssistant:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.conversation_history = []  \n",
    "\n",
    "    def _get_assistant_response(self, prompt):\n",
    "        \n",
    "        try:\n",
    "            completion = openai.ChatCompletion.create(\n",
    "              model=CHAT_MODEL,\n",
    "              messages=prompt,\n",
    "              temperature=0.1\n",
    "            )\n",
    "            \n",
    "            response_message = Message(completion['choices'][0]['message']['role'],completion['choices'][0]['message']['content'])\n",
    "            return response_message.message()\n",
    "            \n",
    "        except Exception as e:\n",
    "            \n",
    "            return f'Request failed with exception {e}'\n",
    "    \n",
    "    # The function to retrieve Redis search results\n",
    "    def _get_search_results(self,prompt):\n",
    "        latest_question = prompt\n",
    "        search_content = get_redis_results(redis_client,latest_question,INDEX_NAME)['result'][0]\n",
    "        return search_content\n",
    "        \n",
    "\n",
    "    def ask_assistant(self, next_user_prompt):\n",
    "        [self.conversation_history.append(x) for x in next_user_prompt]\n",
    "        assistant_response = self._get_assistant_response(self.conversation_history)\n",
    "        \n",
    "        # Answer normally unless the trigger sequence is used \"searching_for_answers\"\n",
    "        if 'let me check on that for you...' in assistant_response['content'].lower():\n",
    "            question_extract = openai.Completion.create(model=COMPLETIONS_MODEL,prompt=f\"Extract the employees' latest question and their role from this conversation: {self.conversation_history}. Extract it as a sentence stating their question question and their role\")\n",
    "            search_result = self._get_search_results(question_extract['choices'][0]['text'])\n",
    "            \n",
    "            # We insert an extra system prompt here to give fresh context to the Chatbot on how to use the Redis results\n",
    "            # In this instance we add it to the conversation history, but in production it may be better to hide\n",
    "            self.conversation_history.insert(-1,{\"role\": 'system',\"content\": f\"Answer the user's question using this content: {search_result}. If you cannot answer the question, say 'Sorry, I don't know the answer to this one. You should call Austin Vance at (970) 306-8100' and he will be happy to provide an answer. He is easiest to reach between the hours of 2am and 4am MDT\"})\n",
    "            #[self.conversation_history.append(x) for x in next_user_prompt]\n",
    "            \n",
    "            assistant_response = self._get_assistant_response(self.conversation_history)\n",
    "            print(next_user_prompt)\n",
    "            print(assistant_response)\n",
    "            self.conversation_history.append(assistant_response)\n",
    "            return assistant_response\n",
    "        else:\n",
    "            self.conversation_history.append(assistant_response)\n",
    "            return assistant_response\n",
    "            \n",
    "        \n",
    "    def pretty_print_conversation_history(self, colorize_assistant_replies=True):\n",
    "        for entry in self.conversation_history:\n",
    "            if entry['role'] == 'system':\n",
    "                pass\n",
    "            else:\n",
    "                prefix = entry['role']\n",
    "                content = entry['content']\n",
    "                output = colored(prefix +':\\n' + content, 'green') if colorize_assistant_replies and entry['role'] == 'assistant' else prefix +':\\n' + content\n",
    "                #prefix = entry['role']\n",
    "                print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101d502c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T18:57:44.923875Z",
     "start_time": "2023-06-01T18:57:41.714842Z"
    }
   },
   "outputs": [],
   "source": [
    "conversation = RetrievalAssistant()\n",
    "messages = []\n",
    "system_message = Message('system',system_prompt)\n",
    "user_message = Message('user','What is a Focused Labs IRL?')\n",
    "messages.append(system_message.message())\n",
    "messages.append(user_message.message())\n",
    "response_message = conversation.ask_assistant(messages)\n",
    "response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702eb4fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T18:57:57.204905Z",
     "start_time": "2023-06-01T18:57:56.452654Z"
    }
   },
   "outputs": [],
   "source": [
    "messages = []\n",
    "user_message = Message('user','I am a designer!')\n",
    "messages.append(user_message.message())\n",
    "response_message = conversation.ask_assistant(messages)\n",
    "#response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f2c812",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T18:57:59.443619Z",
     "start_time": "2023-06-01T18:57:59.389689Z"
    }
   },
   "outputs": [],
   "source": [
    "conversation.pretty_print_conversation_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f9ef37",
   "metadata": {},
   "source": [
    "### Chatbot\n",
    "\n",
    "Now we'll put all this into action with a real (basic) Chatbot.\n",
    "\n",
    "In the directory containing this app, execute ```streamlit run chat.py```. This will open up a Streamlit app in your browser where you can ask questions of your embedded data. \n",
    "\n",
    "__Example Questions__:\n",
    "- what is the cost cap for a power unit in 2023\n",
    "- what should competitors include on their application form\n",
    "- how can a competitor be disqualified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e6c4ca",
   "metadata": {},
   "source": [
    "### Consolidation\n",
    "\n",
    "Over the course of this notebook you have:\n",
    "- Laid the foundations of your product by embedding our knowledge base\n",
    "- Created a Q&A application to serve basic use cases\n",
    "- Extended this to be an interactive Chatbot\n",
    "\n",
    "These are the foundational building blocks of any Q&A or Chat application using our APIs - these are your starting point, and we look forward to seeing what you build with them!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
