from langchain.agents.self_ask_with_search.output_parser import SelfAskOutputParser
from langchain.chat_models import ChatOpenAI
from llama_index import LLMPredictor, ServiceContext


def get_llm_predictor():
    return LLMPredictor(llm=ChatOpenAI(temperature=0, max_tokens=512))


def get_service_context():
    llm_predictor_chatgpt = get_llm_predictor()
    return ServiceContext.from_defaults(llm_predictor=llm_predictor_chatgpt)


def output_response(response) -> None:
    """
    You may be wondering why aren't we streaming the response using the openai completion API
    This is currently in beta in the langchain library, I will update this example
    to showcase this as implementation details may change
    Since it's flagged as beta adding it here may cause confusion as most
    likely it will be changed again within a few weeks
    For now output_response will simulate streaming for the purpose of illustration
    Args:
        response: text output generated by ChatGPT
    """
    if not response:
        print("There's no response.")
    else:
        print(response)
    # for line in textwrap.wrap(response, width=75):
    #     for word in line.split():
    #         for char in word:
    #             print(char, end='', flush=True)
    #             time.sleep(0.01)  # Add a delay of 0.1 seconds between each character
    #         print(' ', end='', flush=True)  # Add a space between each word
    #     print()  # Move to the next line after each line is printed
    print("-----")
