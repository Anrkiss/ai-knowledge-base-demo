from langchain.chat_models import ChatOpenAI
from llama_index import LLMPredictor, ServiceContext


def get_llm_predictor():
    return LLMPredictor(llm=ChatOpenAI(temperature=0, max_tokens=512))


def strip_parse_error_message(error_message):
    return str(error_message).replace("Could not parse LLM output: ", "")

def get_service_context():
    llm_predictor_chatgpt = get_llm_predictor()
    return ServiceContext.from_defaults(llm_predictor=llm_predictor_chatgpt)


def output_response(response) -> None:
    """
    You may be wondering why aren't we streaming the response using the openai completion API
    This is currently in beta in the langchain library, I will update this example
    to showcase this as implementation details may change
    Since it's flagged as beta adding it here may cause confusion as most
    likely it will be changed again within a few weeks
    For now output_response will simulate streaming for the purpose of illustration
    Args:
        response: text output generated by ChatGPT
    """
    if not response:
        print("There's no response.")
    else:
        print(response)
    print("-----")
