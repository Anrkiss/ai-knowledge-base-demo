import json

from langchain.chat_models import ChatOpenAI
from llama_index import LLMPredictor, ServiceContext

from config import CHAT_MODEL


def output_response(response) -> None:
    """
    You may be wondering why aren't we streaming the response using the openai completion API
    This is currently in beta in the langchain library, I will update this example
    to showcase this as implementation details may change
    Since it's flagged as beta adding it here may cause confusion as most
    likely it will be changed again within a few weeks
    For now output_response will simulate streaming for the purpose of illustration
    Args:
        response: text output generated by ChatGPT
    """
    if not response:
        print("There's no response.")
    else:
        print(response)
    print("-----")


def is_answer_formatted_in_json(answer):
    try:
        json.loads(answer)
        return True
    except ValueError as e:
        return False


def format_quotes_in_json(str):
    return str.replace('"', '\\"').replace("\n", "\\n")
